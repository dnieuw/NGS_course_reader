[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Workshop GREAT-LIFE",
    "section": "",
    "text": "Day 1: Amplicon Based Nanopore Sequencing\nWelcome to day 1 of the data analysis workshop!",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing"
    ]
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "1  Preparation",
    "section": "",
    "text": "1.1 Activating the correct conda software environment\nWe have prepared a software environment for you using the anaconda software management tool. Using conda environments is highly recommended when installing bioinformatics software in linux as it manages all the dependencies of differenct softwares for you.\nActivate the custom made viroscience_env by copying and executing the following code:",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "preparation.html#activating-the-correct-conda-software-environment",
    "href": "preparation.html#activating-the-correct-conda-software-environment",
    "title": "1  Preparation",
    "section": "",
    "text": "conda activate viroscience_env\n\n\n\n\n\n\nNote\n\n\n\nWe are now ready to start executing the code to perform quality control of our raw Nanopore sequencing data in the next chapter.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "dataQC.html",
    "href": "dataQC.html",
    "title": "2  Raw Data Quality Control",
    "section": "",
    "text": "2.1 Merging fastq files\nAny file in linux can be pasted to another file using the cat command. zcat in addition also unzips gzipped files (e.g. .fastq.gz extension). If your files are already unzipped, use cat instead.\nModify and run:",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC.html#merging-fastq-files",
    "href": "dataQC.html#merging-fastq-files",
    "title": "2  Raw Data Quality Control",
    "section": "",
    "text": "zcat {folder}/*.fastq.gz &gt; {output}\n\n{folder} should contain all your .fastq.gz files for a single barcode.\n{output} should be the name of the combined unzipped fastq file (e.g. all_barcode01.fastq).",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC.html#running-fastp-quality-controlling-software",
    "href": "dataQC.html#running-fastp-quality-controlling-software",
    "title": "2  Raw Data Quality Control",
    "section": "2.2 Running fastp quality controlling software",
    "text": "2.2 Running fastp quality controlling software\nThe fastp software is a very fast multipurpose quality control software to perform quality and sequence adapter trimming for Illumina short-read and Nanopore long-read data.\nBecause we are processing Nanopore data several quality control options have to be disabled. The only requirement we set is a minimum median phred quality score of the read of 10 and a minimum length of around the size of the amplicon (e.g. 400 nucleotides).\nModify and run:\nfastp -i {input} -o {output} -j /dev/null -h {report} \\\n--disable_trim_poly_g \\\n--disable_adapter_trimming \\\n--qualified_quality_phred 10 \\\n--unqualified_percent_limit 50 \\\n--length_required {min_length} \\\n-w {threads}\n\n{input} is the merged file from the previous section.\n{output} is the the quality controlled .fastq filename (e.g. all_barcode01_QC.fastq).\n{report} is the QC report filename, containing various details about the quality of the data before and after processing.\n{min_length} is the expected size of your amplicons, to remove very short “rubbish” reads, generally the advise is to set it a bit lower than the expected size. Based on the QC report, which lists the number of removed reads you may adjust this setting, if too many reads are removed.\n\n\n\n\n\n\n\nNote\n\n\n\n{threads} is a recurring setting for the number of CPUs to use for the processing. On a laptop this will be less (e.g. 8), on an HPC you may be able to use 64 or more CPUs for processing. However, how much performance increase you get depends on the software.\nSince we are running this on a laptop, please set the threads to 18 for all the following sections.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC.html#trimming-primers-irrespective-of-primer-sequence",
    "href": "dataQC.html#trimming-primers-irrespective-of-primer-sequence",
    "title": "2  Raw Data Quality Control",
    "section": "2.3 Trimming primers irrespective of primer sequence",
    "text": "2.3 Trimming primers irrespective of primer sequence\n\n\n\n\n\n\nNote\n\n\n\nThis is optional if primer sequences are not known.\n\n\nWith amplicon-based sequencing in principle every read contains the sequences of the primers at the start and the end of the read. We can use cutadapt to trim the first and last 30 nucleotides (-u 30 -u -30) of every read to remove the primer sequence.\ncutadapt -u 30 -u -30 -o {output} {input} -j {threads}\n\n{output} is the filename of the primer trimmed fastq.\n{input} is the fastq result of the previous QC step.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC.html#mapping-reads-to-primer-reference",
    "href": "dataQC.html#mapping-reads-to-primer-reference",
    "title": "2  Raw Data Quality Control",
    "section": "2.4 Mapping reads to primer reference",
    "text": "2.4 Mapping reads to primer reference\nTo precisely trim the primers we map the reads to a reference sequence based on which the primers were designed. This is to make sure, when looking for the primer locations, all primer location can be found. To map the reads we use minimap2 with the -x map-ont option for ONT reads. -Y ensures reads are not hardclipped. Afterwards we use samtools to reduce the .bam (mapping) file to only those reads that mapped to the reference and sort the reads in mapping file based on mapping position, which is necessary to continue working with the file.\nminimap2 -Y -t {threads} -x map-ont -a {reference} {input} | \\\nsamtools view -bF 4 - | samtools sort -@ {threads} - &gt; {output}\n\n{reference} is the fasta file containing the reference that your primers should be able to map to.\n{input} is either the trimmed fastq file from cutadapt, or the QC fastq file.\n{output} is the mapping file, it could be named something like barcode01_QCmapped.bam",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC.html#trimming-primers-using-ampliclip",
    "href": "dataQC.html#trimming-primers-using-ampliclip",
    "title": "2  Raw Data Quality Control",
    "section": "2.5 Trimming primers using Ampliclip",
    "text": "2.5 Trimming primers using Ampliclip\nAmpliclip is a tool I wrote to remove the primer sequences of nanopore amplicon reads. It works by mapping the primer sequences to a reference genome to find their location. Then it clips the reads mapped to the same reference (which we did in the previous step) by finding overlap between the primer location and the read ends. It allows for some “junk” in front of the primer location with --padding and mismatches between primer and reference --mismatch. After clipping it trims the reads and outputs a clipped .bam file and a trimmed .fastq file. --minlength can be set to remove any reads that, after trimming, have become shorter than this length. Set this to the value that was used in the QC section (e.g. 400).\nAfter the trimming the clipped mapping file has to be sorted again.\nsamtools index {input.mapped}\n\nampliclip \\\n--infile {input.mapped} \\\n--outfile {output.clipped} \\\n--outfastq {output.trimmed} \\\n--primerfile {primers} \\\n--referencefile {reference}\\\n-fwd LEFT -rev RIGHT \\\n--padding 10 --mismatch 2 --minlength {min_length}\n\nsamtools sort {output.clipped}_ &gt; {output.clipped}\nrm {output.clipped}_\n\n{input.mapped} is the mapping file created in the previous step\n{output.clipped} is the mapping file processed to clip the primer sequences off (e.g. barcode01_clipped.bam).\n{output.trimmed} is the trimmed fastq file, this contains all reads mapped to the reference with primer sequences trimmed off (e.g. barcode01_trimmed.bam).\n{primers} is the name of the primer sequence fasta file. Make sure names of the primers have either ‘LEFT’ or ‘RIGHT’ in their name to specify if it is a left or right side primer.\n{reference} is the name of the reference file, this must be the same file as was used for mapping in the previous step.\n{min_length} is the minimum required length of the trimmed reads, set it to the same value as when using fastp.\n\nTo see what has happened in the trimming process we can open the .bam mapping files before and after primer trimming using the visualization tool UGENE, a free and open source version of the software geneious.\nYou can open UGENE by double-clicking ugeneui in the /home/ugene-50.0/ folder on your laptop.\nIn UGENE you can open a .bam via the “open file” option.\n\n\n\n\n\n\nNote\n\n\n\nWe now have our quality controlled sequence reads which we can use to create a consensus sequence in the next chapter.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "consensus.html",
    "href": "consensus.html",
    "title": "3  Generating a Consensus Sequence",
    "section": "",
    "text": "3.1 Mapping trimmed reads to reference\nSimilar to what we did before we now map the trimmed reads to our preferred reference genome.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating a Consensus Sequence</span>"
    ]
  },
  {
    "objectID": "consensus.html#mapping-trimmed-reads-to-reference",
    "href": "consensus.html#mapping-trimmed-reads-to-reference",
    "title": "3  Generating a Consensus Sequence",
    "section": "",
    "text": "Note\n\n\n\nThis is optional if the reference for the primer design and the preferred reference for consensus generation are different. Otherwise simply use the clipped mapping file from the previous step.\n\n\n\nminimap2 -Y -t {threads} -x map-ont -a {reference} {input} | \\\nsamtools view -bF 4 - | samtools sort -@ {threads} - &gt; {output}\n\n{reference} is the fasta file containing the preferred reference.\n{input} is the trimmed fastq file from the previous step.\n{output} is the mapping file, it could be named something like barcode01_mapped.bam",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating a Consensus Sequence</span>"
    ]
  },
  {
    "objectID": "consensus.html#creating-consensus-from-filtered-mutations",
    "href": "consensus.html#creating-consensus-from-filtered-mutations",
    "title": "3  Generating a Consensus Sequence",
    "section": "3.2 Creating consensus from filtered mutations",
    "text": "3.2 Creating consensus from filtered mutations\nVirconsens is a tool I wrote to create a consensus sequence from Nanopore amplicon reads mapped to a reference in a mapping .bam file.\nIt works by reading the mapping file position-by-position and counting the mutations, insertions and deletions. The mutation or deletion (or original nucleotide from the reference) with the highest count is considered the “consensus” at that position.\nIn the next step it filters positions with too low coverage based on the mindepth threshold or too low frequency based on the minAF threshold.\nThe last step of the tool is to iterate over the reference genome and replace the reference nucleotide with the mutation, insertion or deletion, replace filtered position with “N”, or keep the original reference nucleotide. (1 and 2 nucleotide indels are ignored as they are very often erroneous).\nBefore running virconsens we have to index the .bam mapping file.\nsamtools index {input}\n\nvirconsens \\\n-b {input} \\\n-o {output} \\\n-n {name} \\\n-r {reference} \\\n-d 30 \\\n-af 0.1 \\\n-c {threads}\n\n{input} is the mapping bam file from the previous step.\n{output} is the fasta file containing the consensus sequence (e.g. barcode01_consensus.fasta)\n{name} is the custom name of your sequence that will be used in the fasta file (e.g. barcode01_consensus)\n{reference} is the fasta file containing the preferred reference, the same as in the previous step.\n\n\n\n\n\n\n\nNote\n\n\n\nWe now have a consensus sequence of our sequencing result. This is the “raw” result we can continue using for multiple sequence alignment and phylogeny in the next chapter.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Generating a Consensus Sequence</span>"
    ]
  },
  {
    "objectID": "interpretation.html",
    "href": "interpretation.html",
    "title": "4  Comparing sequence to reference sequences",
    "section": "",
    "text": "4.1 Creating multiple sequence alignment\nWe can use a command to past the consensus sequence we generated to a fasta file of reference sequences. Now is also a good time to open the tool alignement visualization tool Aliview and have a look at our sequence and the alignment.\nPlease find the other mpox sequence .fasta files in the folder “sequences” and use the command below to add them them to the newly generated consensus .fasta file.\nWe now have two options to create a multiple sequence alignment (MAF). (Not to be confused with a read alignment .bam file).\nThe first option is to use a multiple sequence alignment tool such as MAFFT to create a multiple sequence alignment. This is a good option if the genome of your to-be-aligned virus is not very big (e.g. 20kb) and you do not have too many (e.g. 10,000+) genome to align.\nThe second option is to use a reference based multiple sequence alignment approach, which we can do using minimap2 and gofasta. This is very fast and works well even for large genomes (e.g. 200kb+) or many sequences (10,000+). However, gofasta does not perform a “real” multiple alignment, because it ignores insertions in the sequences compared to the reference and removes them. Therefore if insertions are expected and present in the sequences, they will have to be added manually. On the positive side, phylogenetic analysis tools, such as the one we will use below, also ignore any insertions, so for the phylogenetic analysis the removal of insertions does not matter.\nUse one of the two options below to create the multiple sequence alignment:\nOption 1:\nOption 2:\n(tmp.sam can be deleted)",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing sequence to reference sequences</span>"
    ]
  },
  {
    "objectID": "interpretation.html#creating-multiple-sequence-alignment",
    "href": "interpretation.html#creating-multiple-sequence-alignment",
    "title": "4  Comparing sequence to reference sequences",
    "section": "",
    "text": "cat {consensus} {reference} &gt; consensus_with_ref.fasta\n\n\n\n\n\n\nTip\n\n\n\nYou can also use Aliview to copy and paste sequences if you prefer. However, adding together multiple single .fasta files can be cumbersome, so in that case cat {file1}.fasta {file2}.fasta {file3}.fasta &gt; all_consensus.fasta or even better cat *.fasta &gt; all_consensus.fasta can be very convenient to paste together many sequences in one file.\n\n\n\n\n\n\n\nmafft --auto --threads {threads} {input} &gt; {output}\n\nminimap2 -t {threads} -a \\\n-x asm20 \\\n--sam-hit-only \\\n--secondary=no \\\n--score-N=0 \\\n{reference} \\\n{input} \\\n-o tmp.sam\n\ngofasta sam toMultiAlign \\\n-s tmp.sam \\\n-o {output}\n\n{input} here is the .fasta file containing all consensus sequences and references you would like to align.\n{output} is the name of the aligned fasta file (e.g. consensus_with_ref_aligned.fasta).\n{reference} is the reference used to performe a reference based multiple alignment, use the same reference as we used for read mapping before.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAny of .fasta files can be viewed using Aliview to see how the sequences or the alignment looks.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing sequence to reference sequences</span>"
    ]
  },
  {
    "objectID": "interpretation.html#running-iqtree2-maximum-likelyhood-tree-software",
    "href": "interpretation.html#running-iqtree2-maximum-likelyhood-tree-software",
    "title": "4  Comparing sequence to reference sequences",
    "section": "4.2 Running IQTREE2 maximum likelyhood tree software",
    "text": "4.2 Running IQTREE2 maximum likelyhood tree software\nWe can fill a complete course to explain phylogenetic analysis, therefore, here we will just run an maximum likelihood (ML) phylogenetic tree software with our created alignment and look at the result.\nThe software iq-tree is a very powerful tool for phylogenetic analysis. We can simply input our multiple sequence alignment and let iq-tree determine the ML model automatically using “model-finder” and perform 1000 fast bootstraps (-B 1000) to get a phylogenetic tree as an output. -czb (meaning “collapse zero branches”) will make a tree with many identical sequences more readable by removing/collapsing artificial splits between identical sequences.\niqtree -s {input} -B 1000 -nt {threads} -czb\n\n{input} is the multiple sequence alignment file we made in the previous step. The output will be named based on the name of the input file automatically.\n\nAfter iqtree has finished, it produces a file ending in .treefile. This is a “nexus” format phylogenetic tree file which can be opened and visualized using (among others) the tool figtree. This tool allows for zooming in, editing, and annotating phylogenetic trees.\n\n\n\n\n\n\nNote\n\n\n\nPlease open Figtree, load your phylogenetic tree file, and look at your hard work of today :-)!",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Comparing sequence to reference sequences</span>"
    ]
  },
  {
    "objectID": "automation.html",
    "href": "automation.html",
    "title": "5  Automating data analysis",
    "section": "",
    "text": "5.1 Running the automated Nanopore amplicon analysis workflow\nUntil now we have manually ran analysis tools by copying and pasting code step-by-step. This is a great exercise to get a in-depth understanding of how the raw data is processed and how the consensus sequence of a amplicon dataset is generated. However to analyse multiple or many samples at the same time it is cumbersome to do it in this manner.\nTo automate the analysis we have developed a Snakemake workflow which runs all the steps we have manually done before in an automated way and in parallel for all samples we provide.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "automation.html#preparing-to-run-the-workflow",
    "href": "automation.html#preparing-to-run-the-workflow",
    "title": "5  Automating data analysis",
    "section": "5.2 Preparing to run the workflow",
    "text": "5.2 Preparing to run the workflow\nThe workflow takes an input “sample_config” tabular file and based on this file it will process all samples in parallel and automatically.\nThe tabular config file has the following structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniqueID\nFASTQ_path\nReference\nprimers\nPrimer_reference\nSequence_name\ngzipped\ncoverage\nmin_length\n\n\n\n\nBC01\nbarcode01\nNC_003310.fasta\nlong_amplicon_primers.fasta\nNC_063383.1.fasta\nconsensus_barcode01\nTRUE\n30\n1000\n\n\nBC02\nbarcode02\nNC_003310.fasta\nlong_amplicon_primers.fasta\nNC_063383.1.fasta\nconsensus_barcode02\nTRUE\n30\n1000\n\n\n\n\n\n\n\n\n\nAttention!\n\n\n\nMake sure that the first line contains the exact headers as shown in the table above, you can find an example file in the course material.\n\n\n\nUniqueID: This is the unique name used in the workflow to keep track of your sample while processing, please use a simple and unique name (e.g. BC01, etc.)\nFASTQ_path: This is the folder location of all raw .fastq.gz file for a single sample, no need to use cat to paste them together.\nReference: This is the location of the reference sequence .fasta file used for the consensus generation.\nprimers: This is the location of the file containing the primer sequences.\nPrimer_reference: This is the location of the reference sequence .fasta file used for primer trimming.\nSequence_name: This is the name given to the consensus sequence at the end of the pipeline.\ngzipped: Not used yet in the current version of the workflow.\ncoverage: This is the minimum coverage required, anything lower than 30 is not recommended, for low accuracy basecalling, higher coverage is recommended.\nmin_length: This is the minimum length required for the reads to be accepted. This must be below the expected size of the amplicon, for example, for the 2500nt mpox amplicon we use a threshold of 1000.\n\nAfter filling out the tablular file, we can create a directory (folder) for the results with the following process.\nFirst check your current directory with the pwd command:\npwd\nChange your current directory using cd if needed:\ncd /{folder1}/{folder2}\nThen create the new directory using the mkdir command:\nmkdir results\nThis creates a new directory at the current location. Move the sample_config.tsv file to the results directory using the mv (move) command:\nmv sample_config.tsv results\n\n\n\n\n\n\nTip\n\n\n\nIf you get a “file not found” error after changing your directory (cd) you may need to write the complete path of your sample_config.tsv file e.g mv /home/david/Documents/sample_config.tsv results\n\n\nAfter preparing the results directory we have to make sure that we have activated the viroscience_env conda environment and check if the command line has (viroscience_env) in front.\nNext, run the ls command to list the files in the current directory and check if the amplicon_workflow.smk file is present. This is the “recipe” for the workflow, describing all the steps we have done by hand. (you can open the .smk file with a text editor and have a look).",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "automation.html#running-the-workflow",
    "href": "automation.html#running-the-workflow",
    "title": "5  Automating data analysis",
    "section": "5.3 Running the workflow",
    "text": "5.3 Running the workflow\nWe can do a last optional check if everything is fine and the workflow can be executed by performing a “dryrun” using the --dryrun parameter:\nsnakemake \\\n--snakefile \\\namplicon_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} \\\n--cores {threads} \\\n--dryrun\n\n{outdir}: This is the directory we have just created (e.g. results)\n{config}: This is the config file we have created previously.\n\nIf there are no errors we can execute the full workflow by excluding the --dryrun parameter:\nsnakemake \\\n--snakefile \\\namplicon_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} \\\n--cores {threads}\n\n\n\n\n\n\nNote\n\n\n\nThe workflow, with a single sample, should finish in a few minutes on your laptop!",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "amplicon_hpc.html",
    "href": "amplicon_hpc.html",
    "title": "6  Amplicon data analysis on the HPC",
    "section": "",
    "text": "6.1 Connecting to a server\nIf we have an HPC (high performance computer) available and want to analyze a lot of sequence data and do it fast, we can perform the same steps as we have in the previous chapter, but first connect to the HPC.\nDuring this course and the duration of the GREAT-LIFE project we have access to the KCRI HPC. We can login to the HPC using the ssh command:\nOn the KCRI HPC we have prepared the same conda environment as we have used locally. So the same steps have to be done as in the previous chapter to analyze your data.\nA challenge is to transport the raw sequence data to the HPC. We can transfer data using the graphical user interface prepared by KCRI or by using the “+ Other Locations” button in the File browser of your UBUNTU laptop and filling in sftp://{username}@hpc.kcri.it:7076.\nAlso make sure to copy the sample_config.tsv file to the server.\nAfter setting everything up we can redo the analysis at the HPC:",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon data analysis on the HPC</span>"
    ]
  },
  {
    "objectID": "amplicon_hpc.html#connecting-to-a-server",
    "href": "amplicon_hpc.html#connecting-to-a-server",
    "title": "6  Amplicon data analysis on the HPC",
    "section": "",
    "text": "ssh {username}@hpc.kcri.it -p 7076\n\n{username}: Your personal username, be aware, you also need to know your password.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease wait for instructions on how to setup your environment and where to run your analysis!\n\n\n\nsnakemake \\\n--snakefile \\\namplicon_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} \\\n--cores {threads}\n\n{outdir}: This is the directory we have just created (e.g. results)\n{config}: This is the config file we have created previously.",
    "crumbs": [
      "Day 1: Amplicon Based Nanopore Sequencing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon data analysis on the HPC</span>"
    ]
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Day 2: Metagenomic Nanopore Sequencing",
    "section": "",
    "text": "Welcome to day 2 of the data analysis workshop!",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing"
    ]
  },
  {
    "objectID": "preparation2.html",
    "href": "preparation2.html",
    "title": "7  Preparation",
    "section": "",
    "text": "Important!\n\n\n\nMake sure you have activated the custom made viroscience_env again by copying and executing the following code:\nconda activate viroscience_env\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe are now ready to start executing the code to perform quality control of our raw Nanopore sequencing data in the next chapter.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "dataQC2.html",
    "href": "dataQC2.html",
    "title": "8  Raw Data Quality Control",
    "section": "",
    "text": "8.1 Merging fastq files\nThe following steps are the similar for metagenomic Nanopore data as for amplicon data.\nAny file in linux can be pasted to another file using the cat command. zcat in addition also unzips gzipped files (e.g. .fastq.gz extension). If your files are already unzipped, use cat instead.\nModify and run:",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC2.html#merging-fastq-files",
    "href": "dataQC2.html#merging-fastq-files",
    "title": "8  Raw Data Quality Control",
    "section": "",
    "text": "zcat {folder}/*.fastq.gz &gt; {output}\n\n{folder} should contain all your .fastq.gz files for a single barcode.\n{output} should be the name of the combined unzipped fastq file (e.g. all_barcode01.fastq).",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC2.html#trimming-metagenome-adapters-and-random-nonamer-sequences",
    "href": "dataQC2.html#trimming-metagenome-adapters-and-random-nonamer-sequences",
    "title": "8  Raw Data Quality Control",
    "section": "8.2 Trimming metagenome adapters and random nonamer sequences",
    "text": "8.2 Trimming metagenome adapters and random nonamer sequences\nBy using the SISPA library preparation approach we have added sequence adapters and random nonamer sequences to our library fragments. Before we continue with our data we have to remove these adapters and nonamers by using the cutadapt software again.\nFirst we trim the adapter sequence “GTTTCCCACTGGAGGATA” and the reverse complement “TATCCTCCAGTGGGAAAC” at the end of the read. We allow 0.2 errors (20%) and remove up to 5 adapters if more are present (n). We also remove any reads shorter than 150 nucleotides.\nNext we remove 9 nucleotides from each read from the front and the end.\nWe remove the intermediate product “tmp.fastq” with rm.\ncutadapt -j 18 -e 0.2 -n 5 -m 150 --revcomp -a GTTTCCCACTGGAGGATA...TATCCTCCAGTGGGAAAC {input} &gt; tmp.fastq\ncutadapt -j 18 -u 9 -u -9 tmp.fastq &gt; {output}\nrm tmp.fastq\n\n{input} is the merged file from the previous step.\n{output} name you give to the adapter trimmed .fastq file (e.g. all_barcode01_trimmed.fastq).",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC2.html#running-fastp-quality-controlling-software",
    "href": "dataQC2.html#running-fastp-quality-controlling-software",
    "title": "8  Raw Data Quality Control",
    "section": "8.3 Running fastp quality controlling software",
    "text": "8.3 Running fastp quality controlling software\nThe fastp software can also be used to QC metagenomic reads.\nBecause we are processing Nanopore data several quality control options have to be disabled. The only requirement we set is a minimum median phred quality score of the read of 10.\nfastp -i {input} -o {output} -j /dev/null -h {report} \\\n--disable_trim_poly_g \\\n--disable_adapter_trimming \\\n--qualified_quality_phred 10 \\\n--unqualified_percent_limit 50 \\\n-w {threads}\n\n{input} is the adapter trimmed file from the previous step.\n{output} is the the quality controlled .fastq filename (e.g. all_barcode01_QC.fastq).\n{report} is the QC report filename, containing various details about the quality of the data before and after processing.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "dataQC2.html#removing-reads-belonging-to-the-host-e.g.-human-dna",
    "href": "dataQC2.html#removing-reads-belonging-to-the-host-e.g.-human-dna",
    "title": "8  Raw Data Quality Control",
    "section": "8.4 Removing reads belonging to the host (e.g. Human DNA)",
    "text": "8.4 Removing reads belonging to the host (e.g. Human DNA)\nIt is often good to remove any host related reads from your dataset before assembly. In some cases there are many host related sequences in your data which makes the assembly step much more compute intensive and removing the host sequences speeds up analysis. It can also avoid misinterpretation of annotations, which can happen if there are mistakes in the viral reference genomes or if a conserved protein is similar to that in the human genome (e.g. Herpes proteins).\nTo remove the human reads in this case, we map the QCed reads to the human HG38 reference genome and keep only those reads that do not map.\nminimap2 -aY -t {threads} -x map-ont {params.reference} {input} | samtools fastq -f 4 - &gt; {output}\n\n{reference} is the location of the HG38 reference file.\n{input} is the QC .fastq file from the previous step.\n{output} is the filtered .fastq filename without human reads (e.g. all_barcode01_nohuman.fastq).\n\nBecause we did not use any amplicon primers we do not need to trim any primers from the reads.\n\n\n\n\n\n\nNote\n\n\n\nWe now have our quality controlled sequence reads which we can use perform a de-novo assembly in the next chapter.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Raw Data Quality Control</span>"
    ]
  },
  {
    "objectID": "assembly_annotation.html",
    "href": "assembly_annotation.html",
    "title": "9  Assembly and annotation",
    "section": "",
    "text": "9.1 Nanopore sequence assembly\nFor the assembly of our quality controlled reads we are going to use Flye assembler meta.\nAs written on their website: “Flye is a de novo assembler for single-molecule sequencing reads, such as those produced by PacBio and Oxford Nanopore Technologies. It is designed for a wide range of datasets, from small bacterial projects to large mammalian-scale assemblies. The package represents a complete pipeline: it takes raw PacBio / ONT reads as input and outputs polished contigs. Flye also has a special mode for metagenome assembly.”\nWe will use the “special metagenome mode to assemble our viral sequence data.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembly and annotation</span>"
    ]
  },
  {
    "objectID": "assembly_annotation.html#nanopore-sequence-assembly",
    "href": "assembly_annotation.html#nanopore-sequence-assembly",
    "title": "9  Assembly and annotation",
    "section": "",
    "text": "flye --nano-raw {input} --threads 18 --meta -o {output}\n\n{input} This is the QCed read file from the previous section.\n{output} This is the name of the folder where Flye will produce all its files and a “assembly.fasta” file containing the generate contigs.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembly and annotation</span>"
    ]
  },
  {
    "objectID": "assembly_annotation.html#mapping-reads-back-to-assembled-contigs",
    "href": "assembly_annotation.html#mapping-reads-back-to-assembled-contigs",
    "title": "9  Assembly and annotation",
    "section": "9.2 Mapping reads back to assembled contigs",
    "text": "9.2 Mapping reads back to assembled contigs\nTo get an idea of how many reads have been used to generate the assembled contigs we can map the QCed reads back to the contigs. With Illumina sequencing data this method can be used to get an idea of how much of the specific organism was present in the sample, because it has a somewhat linear relationship. For Nanopore sequencing data this relationship between the number of reads and the amount of virus in the sample is not clear.\nWe use minimap2 again to map the reads to the contigs with the following command:\nminimap2 -aY -t {threads} -x map-ont {input.assembly} {input.reads} | samtools view -bF 4 - | samtools sort -@ {threads} - &gt; {output}\n\n{input.assembly} This is the assembly.fasta generated by Flye.\n{input.reads} This is the QCed read file from the previous section.\n{output} This is the .bam file with mapped reads to all assembled contigs.\n\nYou can now open the .bam file using UGENE and look at the coverage of the assembled contigs.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembly and annotation</span>"
    ]
  },
  {
    "objectID": "assembly_annotation.html#nanopore-sequence-annotation",
    "href": "assembly_annotation.html#nanopore-sequence-annotation",
    "title": "9  Assembly and annotation",
    "section": "9.3 Nanopore sequence annotation",
    "text": "9.3 Nanopore sequence annotation\nTo keep the annotation quick and simple and enable everyone to do the annotation on their laptop we have prepared a small annotation database containing all complete human virus sequences from NCBI (without SARS-CoV-2). We have also created a BLAST database from these sequences to annotate our contig sequences.\nThe following command will use the local blastn software to find matches between our sequences and the custom database.\nThe outfmt parameter describes the tab-separated file format and the columns in that file. For a full description of all the output please refer to this website.\nblastn -db {db} -query {input} -out {output} -outfmt '6 qseqid sseqid pident qlen slen length mismatch gapopen qstart qend sstart send evalue bitscore staxids' -num_threads {threads}\n\n{db} This is the location of the blastdb (ncbi_viral_complete_human.fasta)\n{output} This is the name of the folder where Flye will produce all its files and a “assembly.fasta” file containing the generate contigs.\n{input} This is the QCed read file from the previous section.\n{output} This is the name of the folder where Flye will produce all its files and a “assembly.fasta” file containing the generate contigs.\n\n\n\n\n\n\n\nNote\n\n\n\nWe have now completed the initial processing and analysis of our metagenomic sequencing dataset. Please take your time to go over all the intermediate and result files and ask any questions! Then next step is automating the analysis using Snakemake.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembly and annotation</span>"
    ]
  },
  {
    "objectID": "automation2.html",
    "href": "automation2.html",
    "title": "10  Automating data analysis",
    "section": "",
    "text": "10.1 Running the automated Nanopore metagenomic analysis workflow\nSimilar to the amplicon workflow we have created a Snakemake workflow for the metagenomic analysis steps from the previous section.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "automation2.html#preparing-to-run-the-workflow",
    "href": "automation2.html#preparing-to-run-the-workflow",
    "title": "10  Automating data analysis",
    "section": "10.2 Preparing to run the workflow",
    "text": "10.2 Preparing to run the workflow\nThe workflow takes an input “sample_config” tabular file and based on this file it will process all samples in parallel and automatically.\nThe tabular config file has the following structure:\n\n\n\nUniqueID\nFASTQ_path\n\n\n\n\nBC01\nbarcode01\n\n\nBC02\nbarcode02\n\n\n\n\n\n\n\n\n\nAttention!\n\n\n\nMake sure that the first line contains the exact headers as shown in the table above, you can find an example file in the course material.\n\n\n\nUniqueID: This is the unique name used in the workflow to keep track of your sample while processing, please use a simple and unique name (e.g. BC01, etc.)\nFASTQ_path: This is the folder location of all raw .fastq.gz file for a single sample, no need to use cat to paste them together.\n\nAfter filling out the tablular file, we can create a directory (folder) for the results with the following process.\nFirst check your current directory with the pwd command:\npwd\nChange your current directory using cd if needed:\ncd /{folder1}/{folder2}\nThen create the new directory using the mkdir command:\nmkdir results\nThis creates a new directory at the current location. Move the sample_config.tsv file to the results directory using the mv (move) command:\nmv sample_config.tsv results\n\n\n\n\n\n\nTip\n\n\n\nIf you get a “file not found” error after changing your directory (cd) you may need to write the complete path of your sample_config.tsv file e.g mv /home/david/Documents/sample_config.tsv results\n\n\nAfter preparing the results directory we have to make sure that we have activated the viroscience_env conda environment and check if the command line has (viroscience_env) in front.\nNext, run the ls command to list the files in the current directory and check if the amplicon_workflow.smk, GCF_000001405.26_GRCh38_genomic.fna.gz, ncbi_viral_complete_human.fasta database files and new_taxdump.tar.gz files are present.\n\n\n\n\n\n\nImportant!\n\n\n\nAlso make sure to edit the location of the post_process_annotation.py script to where you have stored in on your computer!\nOpen the amplicon_workflow.smk file, search for post_process_annotation.py and edit the path!",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "automation2.html#running-the-workflow",
    "href": "automation2.html#running-the-workflow",
    "title": "10  Automating data analysis",
    "section": "10.3 Running the workflow",
    "text": "10.3 Running the workflow\nWe can do a last optional check if everything is fine and the workflow can be executed by performing a “dryrun” using the --dryrun parameter:\nsnakemake \\\n--snakefile \\\nviral_metagenome_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} host_reference=../GCF_000001405.26_GRCh38_genomic.fna.gz database=../ncbi_viral_complete_human.fasta taxdump=../new_taxdump.tar.gz \\\n--cores {threads} \\\n--dryrun\n\n{outdir}: This is the directory we have just created (e.g. results)\n{config}: This is the config file we have created previously.\n\nIf there are no errors we can execute the full workflow by excluding the --dryrun parameter:\nsnakemake \\\n--snakefile \\\nviral_metagenome_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} host_reference=../GCF_000001405.26_GRCh38_genomic.fna.gz database=../ncbi_viral_complete_human.fasta taxdump=../new_taxdump.tar.gz \\\n--cores {threads}\n\n\n\n\n\n\nNote\n\n\n\nThe workflow, with a single sample, should finish in a about 20 minutes on your laptop. Next we will have a look at working on the HPC to run the analysis.",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automating data analysis</span>"
    ]
  },
  {
    "objectID": "metagenome_hpc.html",
    "href": "metagenome_hpc.html",
    "title": "11  Metagenomic data analysis on the HPC",
    "section": "",
    "text": "11.1 Connecting to a server\nLogin to the HPC:\nOn the KCRI HPC we have prepared the same conda environment as we have used locally. So the same steps have to be done as in the previous chapter to analyze your data.\nA challenge is to transport the raw sequence data to the HPC. We can transfer data using the graphical user interface prepared by KCRI or by using the “+ Other Locations” button in the File browser of your UBUNTU laptop and filling in sftp://{username}@hpc.kcri.it:7076.\nAlso make sure to copy the sample_config.tsv file to the server.\nAfter setting everything up we can redo the analysis at the HPC:",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metagenomic data analysis on the HPC</span>"
    ]
  },
  {
    "objectID": "metagenome_hpc.html#connecting-to-a-server",
    "href": "metagenome_hpc.html#connecting-to-a-server",
    "title": "11  Metagenomic data analysis on the HPC",
    "section": "",
    "text": "ssh {username}@hpc.kcri.it -p 7076\n\n{username}: Your personal username, be aware, you also need to know your password.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease wait for instructions on how to setup your environment and where to run your analysis!\n\n\n\nsnakemake \\\n--snakefile \\\nviral_metagenome_workflow.smk \\\n--directory {ourdir} \\\n--configfile sample_config={config} host_reference=../GCF_000001405.26_GRCh38_genomic.fna.gz database=../ncbi_viral_complete_human.fasta taxdump=../new_taxdump.tar.gz \\\n--cores {threads}\n\n{outdir}: This is the directory we have just created (e.g. results)\n{config}: This is the config file we have created previously.\n\n\n\n\n\n\n\nNote\n\n\n\nThis concludes the second day of the GREAT-LIFE Data Analysis Workshop!",
    "crumbs": [
      "Day 2: Metagenomic Nanopore Sequencing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metagenomic data analysis on the HPC</span>"
    ]
  }
]